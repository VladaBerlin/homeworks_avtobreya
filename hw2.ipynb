{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Берлин Влада 202\n",
    "# Домашнее задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "\n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "\n",
    "import spacy\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus.txt', encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ru_core_news_sm')\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    "doc = Doc(text)\n",
    "\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "слова, которые, на мой взгляд, могли вызвать сложности: мгу, прям, уггга, завкафедрой, юрфака, побежу, победю, ран, лучче, трепков, житьё-бытьё, повтикали, спбгу, спбпу, ниу вшэ, итмо, скроллинг, наркодиспансер, телеграм-канал, рунета\n",
    "\n",
    "1. такие штуки как мгу, ран - это аббревиатуры, которые могут быть распознаны как глагол (мгу - нач. форма мгить)\n",
    "2. прям - разговорное слово, которое может получить неверный тег\n",
    "3. лучче - честно говоря, не знаю, опечатка это или нет, но это слово явно должно вызвать затруднения у программы\n",
    "4. побежу, победю - разговорные фактически аграмматические формы глагола\n",
    "5. повтикали - глагол, который может вызвать затруднение, тк это явно какое-то диалектное слово (ну или слово другого родственного языка вроде украинского)\n",
    "6. скроллинг, телеграм-канал, рунет - неологизмы, которые не факт, что программа правильно разспознает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ТЭГИ с https://universaldependencies.org/u/pos/\n",
    "но с некоторыми отличиями (например, я не различала CCONJ и SCONJ, у меня просто CONJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "я взяла именно этот набор тегов, так как он наиболее универсальный и удобный (к тому же этот набор используют два из трех выбранных мной теггеров)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "работаем с natasha, pymorphy, spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "проверка на то, одинаковая ли длина у автоматической и ручной разметок, если нет, то делаем так, чтобы она была одинаковой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking(man, auto):\n",
    "    m = man.copy()\n",
    "    a = auto.copy()\n",
    "    for i in range(len(m)):\n",
    "        if m[i][0] != a[i][0]:\n",
    "            if len(m[i][0]) > len(a[i][0]):\n",
    "                m.insert(i+1, m[i][0])\n",
    "                i += 1\n",
    "            else:\n",
    "                a.insert(i+1, a[i][0])\n",
    "                i += 1\n",
    "    return m, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "оставляем только части речи, чтобы сделать проверку на accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_pos(man, auto, dct):\n",
    "    m_pos = []\n",
    "    a_pos = []\n",
    "    for i in range(len(man)):\n",
    "        if auto[i][1] in dct.keys():\n",
    "            m_pos.append(man[i][1])\n",
    "            a_pos.append(dct[auto[i][1]])\n",
    "        else:\n",
    "            m_pos.append(man[i][1])\n",
    "            a_pos.append(auto[i][1])\n",
    "    return m_pos, a_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_ann = []\n",
    "with open('corpus_MAN.txt', encoding='utf8') as f:\n",
    "    t = f.read()\n",
    "\n",
    "toks = t.split(' ')\n",
    "for tok in toks:\n",
    "    man_ann.append([tok.split('_')[0], tok.split('_')[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "словарь перевода теггов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_to_my = {'AUX': 'VERB', 'CCONJ': 'CONJ', 'SCONJ': 'CONJ'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_ann = []\n",
    "\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "\n",
    "for token in doc.tokens:\n",
    "  token.lemmatize(morph_vocab)\n",
    "    \n",
    "for el in doc.tokens:\n",
    "    nat_ann.append([el.text, el.pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "сравнение ручной и автоматической разметок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9173\n"
     ]
    }
   ],
   "source": [
    "man_nat, nat = checking(man_ann, nat_ann)\n",
    "\n",
    "m_n_pos, nat_pos = only_pos(man_nat, nat, nat_to_my)\n",
    "\n",
    "print(\"Accuracy: %.4f\" % accuracy_score(m_n_pos, nat_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "словарь перевода теггов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorphy_to_my = {'ADJF': 'ADJ', 'ADJS': 'ADJ', 'COMP': 'ADJ', 'INFN': 'VERB', 'PRTF': 'VERB', 'PRTS': 'VERB',\n",
    "'GRND': 'VERB', 'NUMR': 'NUM', 'ADVB': 'ADV', 'NPRO': 'PRON', 'PRED': 'ADV', 'PREP': 'ADP', 'PRCL': 'PART'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_ann = []\n",
    "\n",
    "for token in doc.tokens:\n",
    "    if morph.parse(token.text)[0].tag.POS:\n",
    "        pm_ann.append([token.text, morph.parse(token.text)[0].tag.POS])\n",
    "    else:\n",
    "        pm_ann.append([token.text, 'PUNCT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "сравнение ручной и автоматической разметок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8710\n"
     ]
    }
   ],
   "source": [
    "man_pm, pm = checking(man_ann, pm_ann)\n",
    "\n",
    "m_pm_pos, pm_pos = only_pos(man_pm, pm, pymorphy_to_my)\n",
    "\n",
    "print(\"Accuracy: %.4f\" % accuracy_score(m_pm_pos, pm_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как natasha использует те же теги, что и spacy, список для перевода тегов остаётся тем же"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_ann = []\n",
    "\n",
    "for token in doc.tokens:\n",
    "    sp_ann.append([token.text, token.pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "сравнение ручной и автоматической разметок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9173\n"
     ]
    }
   ],
   "source": [
    "man_sp, sp = checking(man_ann, sp_ann)\n",
    "\n",
    "m_sp_pos, sp_pos = only_pos(man_sp, sp, nat_to_my)\n",
    "\n",
    "print(\"Accuracy: %.4f\" % accuracy_score(m_sp_pos, sp_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, что natasha и spacy справились одинаково хорошо, pymorphy немного похуже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.4f\" % accuracy_score(nat_pos, sp_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить, natasha и spacy разметили корпус абсолютно одинако. Я буду использовать natasha по личным предпочтениям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. очень + ADJ\n",
    "2. не + VERB\n",
    "3. ADJ + NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "я выбрала эти ngramm'ы, так как они, на мой взгляд, должны неплохо помогать указывать на полярность отзыва, например, очень + прилагательное будет, скорее всего, может неплохо помочь, так как такие пары должны хорошо указывать на тональность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(text):\n",
    "  ngramms = []\n",
    "  dct = {'ADJF': 'ADJ', 'ADJS': 'ADJ', 'COMP': 'ADJ', 'INFN': 'VERB',\n",
    "  'PRTF': 'VERB', 'PRTS': 'VERB', 'GRND': 'VERB'}\n",
    "\n",
    "  doc = Doc(text)\n",
    "  doc.segment(segmenter)\n",
    "  doc.tag_morph(morph_tagger)\n",
    "\n",
    "  for token in doc.tokens:\n",
    "    token.lemmatize(morph_vocab)\n",
    "\n",
    "  i = 0\n",
    "  l = -2  ## На случай, если какой-то из интересующих нас частеречных тегов, являющийся вторым элементом ngramm'ы, появится раньше, чем первый элемент такой ngramm'ы\n",
    "  k = -2\n",
    "  m = -2\n",
    "\n",
    "  for t in doc.tokens:\n",
    "\n",
    "    if t.pos == 'VERB':\n",
    "      if i == m + 1:              ## Если предыдущее слово является элементом ngramm'ы\n",
    "        mor = morph.parse(t.text)\n",
    "        for el in mor:\n",
    "          if el.tag.POS in dct:\n",
    "            tg = dct[el.tag.POS]\n",
    "          else:\n",
    "            tg = el.tag.POS\n",
    "          if tg == 'VERB':\n",
    "            ngramms[-1].append(el.normal_form)  ## В наших интересах записывать начальные формы, так как в таком случае от них будет больше пользы (если мы говорим о целях, которые мы преследовали в первой домашке)\n",
    "            break\n",
    "          if el == mor[-1]:             ## Если pymorphy не смог разобрать слово как нужную нам часть речи, то приходится добавлять форму, которая встретилась в тексте\n",
    "            ngramms[-1].append(t.text)\n",
    "\n",
    "    elif t.pos == 'NOUN':\n",
    "      if i == k + 1:              ## Если предыдущее слово является элементом ngramm'ы\n",
    "        mor = morph.parse(t.text)\n",
    "        for el in mor:\n",
    "          if el.tag.POS == 'NOUN':\n",
    "            ngramms[-1].append(el.normal_form)\n",
    "            break\n",
    "          if el == mor[-1]:\n",
    "            ngramms[-1].append(t.text)\n",
    "\n",
    "    elif t.pos == 'ADJ':\n",
    "      if i == l + 1:              ## Если предыдущее слово является элементом ngramm'ы\n",
    "        mor = morph.parse(t.text)\n",
    "        for el in mor:\n",
    "          tg = ''\n",
    "          if el.tag.POS in dct:   ## Так как все теги, обозначающие прилагательные, записаны в словаре перевода тегов, можно проверять подходит ли нам этот разбор только по тем тегам, которые есть в словаре\n",
    "            tg = dct[el.tag.POS]\n",
    "          if tg == 'ADJ':\n",
    "            ngramms[-1].append(el.normal_form)\n",
    "            break\n",
    "          if el == mor[-1]:\n",
    "            ngramms[-1].append(t.text)\n",
    "            \n",
    "    elif k == i - 1 or l == i - 1 or m == i - 1:\n",
    "      ngramms = ngramms[:-1]\n",
    "\n",
    "    if t.pos == 'ADJ':\n",
    "      k = i\n",
    "      ngramms.append([t.text])\n",
    "\n",
    "    elif t.text == 'очень':\n",
    "      l = i\n",
    "      ngramms.append([t.text])\n",
    "\n",
    "    elif t.text == 'не':\n",
    "      m = i\n",
    "      ngramms.append([t.text])\n",
    "\n",
    "    i += 1\n",
    "  return ngramms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dc2e76d637f38cb534d007662ee080514e23b3d0d0dd6e2f651136baa4a9222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
