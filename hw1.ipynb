{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# БЕРЛИН ВЛАДА БКЛ-202\n",
    "# Домашнее задание №1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "\n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.session()\n",
    "sw = stopwords.words('russian')\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "nlp = spacy.load('ru_core_news_sm')\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Притворяемся браузером, чтобы собрать данные с сайта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent(verify_ssl=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_texts = []\n",
    "contra_texts = []\n",
    "check_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(text):\n",
    "  ngramms = []\n",
    "  dct = {'ADJF': 'ADJ', 'ADJS': 'ADJ', 'COMP': 'ADJ', 'INFN': 'VERB',\n",
    "  'PRTF': 'VERB', 'PRTS': 'VERB', 'GRND': 'VERB'}\n",
    "\n",
    "  doc = Doc(text)\n",
    "  doc.segment(segmenter)\n",
    "  doc.tag_morph(morph_tagger)\n",
    "\n",
    "  for token in doc.tokens:\n",
    "    token.lemmatize(morph_vocab)\n",
    "\n",
    "  i = 0\n",
    "  l = -2  ## На случай, если какой-то из интересующих нас частеречных тегов, являющийся вторым элементом ngramm'ы, появится раньше, чем первый элемент такой ngramm'ы\n",
    "  k = -2\n",
    "  m = -2\n",
    "\n",
    "  for t in doc.tokens:\n",
    "\n",
    "    if t.pos == 'VERB':              ## Если предыдущее слово является элементом ngramm'ы\n",
    "      if i == m + 1:\n",
    "        mor = morph.parse(t.text)\n",
    "        for el in mor:\n",
    "          if el.tag.POS in dct:\n",
    "            tg = dct[el.tag.POS]\n",
    "          else:\n",
    "            tg = el.tag.POS\n",
    "          if tg == 'VERB':\n",
    "            ngramms[-1].append(el.normal_form)  ## В наших интересах записывать начальные формы, так как в таком случае от них будет больше пользы\n",
    "            break\n",
    "          if el == mor[-1]:             ## Если pymorphy не смог разобрать слово как нужную нам часть речи, то приходится добавлять форму, которая встретилась в тексте\n",
    "            ngramms[-1].append(t.text)\n",
    "\n",
    "    elif t.pos == 'NOUN':\n",
    "      if i == k + 1:              ## Если предыдущее слово является элементом ngramm'ы\n",
    "        mor = morph.parse(t.text)\n",
    "        for el in mor:\n",
    "          if el.tag.POS == 'NOUN':   ## Так как все теги, обозначающие прилагательные, записаны в словаре перевода тегов, можно проверять подходит ли нам этот разбор только по тем тегам, которые есть в словаре\n",
    "            ngramms[-1].append(el.normal_form)\n",
    "            break\n",
    "          if el == mor[-1]:\n",
    "            ngramms[-1].append(t.text)\n",
    "\n",
    "    elif t.pos == 'ADJ':\n",
    "      if i == l + 1:              ## Если предыдущее слово является элементом ngramm'ы\n",
    "        mor = morph.parse(t.text)\n",
    "        for el in mor:\n",
    "          tg = ''\n",
    "          if el.tag.POS in dct:\n",
    "            tg = dct[el.tag.POS]\n",
    "          if tg == 'ADJ':\n",
    "            ngramms[-1].append(el.normal_form)\n",
    "            break\n",
    "          if el == mor[-1]:\n",
    "            ngramms[-1].append(t.text)\n",
    "\n",
    "    elif k == i - 1 or l == i - 1 or m == i - 1:\n",
    "      ngramms = ngramms[:-1]\n",
    "\n",
    "    if t.pos == 'ADJ':\n",
    "      k = i\n",
    "      ngramms.append([t.text])\n",
    "\n",
    "    elif t.text == 'очень':\n",
    "      l = i\n",
    "      ngramms.append([t.text])\n",
    "\n",
    "    elif t.text == 'не':\n",
    "      m = i\n",
    "      ngramms.append([t.text])\n",
    "\n",
    "    i += 1\n",
    "  return ngramms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В разделе, в котором я ищу, всего 8 страниц, поэтому я ввела такой интервал для i.\n",
    "На этом сайте пользователи еще дополнительно пишут достоинства и недостатки, поэтому их я тоже буду использовать для нахождения ключевых слов.\n",
    "Я решила, что положительные отзывы - это отзывы с оценками 5 и 4, а все остальные - это негативные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_check = {}\n",
    "ind = 0\n",
    "pos_ng = []\n",
    "neg_ng = []\n",
    "\n",
    "for i in range(1, 9):\n",
    "    page_number = i\n",
    "\n",
    "    ## Получаем доступ к странице, где представлено сколько-то игр\n",
    "    url = f'https://spasibovsem.ru/igry-dlya-pk-otzyvy/?page={page_number}'\n",
    "    req = session.get(url, headers={'User-Agent': ua.random})\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    ## Получаем доступ к странице с отзывами к какой-то определенной игре \n",
    "    list_games = soup.find_all('div', {'class': 'response-counter adaptive'})\n",
    "    for game_ind in range(len(list_games)):\n",
    "        link = list_games[game_ind].find('a').attrs['href']\n",
    "        game_url = f'https://spasibovsem.ru{link}'\n",
    "        game_req = session.get(game_url, headers={'User-Agent': ua.random})\n",
    "        game_page = game_req.text\n",
    "        game_soup = BeautifulSoup(game_page, 'html.parser')\n",
    "\n",
    "        ## Получаем доступ к странице с определенным отзывом\n",
    "        list_fb = game_soup.find_all('div', {'class': 'name'})\n",
    "        for fb_ind in range(len(list_fb)):\n",
    "            fb_link = list_fb[fb_ind].find('a').attrs['href']\n",
    "            fb_url = f'https://spasibovsem.ru{fb_link}'\n",
    "            fb_req = session.get(fb_url, headers={'User-Agent': ua.random})\n",
    "            fb_page = fb_req.text\n",
    "            fb_soup = BeautifulSoup(fb_page, 'html.parser')\n",
    "            fb_text = fb_soup.find('div', {'class': 'text response-text description'}).text\n",
    "            pro_texts.append(fb_soup.find('span', {'class': 'pro'}).text)\n",
    "            contra_texts.append(fb_soup.find('span', {'class': 'contra'}).text)\n",
    "            rank = fb_soup.find('abbr', {'class': 'rating'}).attrs['title']\n",
    "            ng = chunker(fb_text)\n",
    "            if (rank == '5' or rank == '4') and len(pro_texts) < 61:  ## Так как я добавляю еще и текст из строки \"достоинства\", на 30 текстов будет приходиться 60 элементов списка\n",
    "                pro_texts.append(fb_text)\n",
    "                pos_ng.extend(ng)\n",
    "            elif rank != '5' and rank != '4' and len(contra_texts) < 61: ## То же самое, что и в случае с высокой оценкой\n",
    "                contra_texts.append(fb_text)\n",
    "                neg_ng.extend(ng)\n",
    "            elif len(check_texts) < 11:     ## Если в списке, к которому подходит этот отзыв, уже есть 30 отзывов, и в тестовой выборке еще не собралось 10 текстов, то мы добавляем этот текст в нее\n",
    "                check_texts.append(fb_text)\n",
    "                if rank == '5' or rank == '4':\n",
    "                    for_check[ind] = 'pro'\n",
    "                    pos_ng.extend(ng)\n",
    "                else:\n",
    "                    for_check[ind] = 'contra'\n",
    "                    neg_ng.extend(ng)\n",
    "                ind += 1\n",
    "            if len(check_texts) > 9 and len(pro_texts) > 60 and len(contra_texts) > 60:\n",
    "                break\n",
    "        if len(check_texts) > 9 and len(pro_texts) > 60 and len(contra_texts) > 60:\n",
    "                break\n",
    "    if len(check_texts) > 9 and len(pro_texts) > 60 and len(contra_texts) > 60:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "нахождение ключевых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "contra_prep = []\n",
    "pro_prep = []\n",
    "\n",
    "for text in contra_texts:\n",
    "    tokenized_text = [w.lower() for w in word_tokenize(text) if w.isalpha()]\n",
    "    filt_words = [w for w in tokenized_text if w not in sw]\n",
    "    lemmas = [morph.parse(w)[0].normal_form for w in filt_words]\n",
    "    contra_prep.extend(lemmas)\n",
    "    contra_count = Counter(contra_prep) # Считаем сколько раз встретилось каждое слово\n",
    "    \n",
    "contra_prep = list(set(contra_prep))\n",
    "\n",
    "for text in pro_texts:\n",
    "    tokenized_text = [w.lower() for w in word_tokenize(text) if w.isalpha()]\n",
    "    filt_words = [w for w in tokenized_text if w not in sw]\n",
    "    lemmas = [morph.parse(w)[0].normal_form for w in filt_words]\n",
    "    pro_prep.extend(lemmas)\n",
    "    pro_count = Counter(pro_prep)\n",
    "\n",
    "pro_prep = list(set(pro_prep))\n",
    "\n",
    "for w in contra_prep:\n",
    "    if w in pro_prep:\n",
    "        contra_prep.remove(w)   # Убираем, если слово встречается и в положительных, и в отрицательных отзывах\n",
    "        pro_prep.remove(w)\n",
    "    elif contra_count[w] < 10:  #Я решила брать только те слова, которые встретились 10 раз или больше\n",
    "        contra_prep.remove(w)\n",
    "\n",
    "for w in pro_prep:\n",
    "    if pro_count[w] < 10:\n",
    "        pro_prep.remove(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определяем положительный или отрицательный перед нами отзыв и в конце проверяем accuracy того, что у нас получилось. За каждое слово, которое встречается в положительных отзывах, мы добавляем 1 к \"оценке\", а за слово, которое встречается в отрицательных отзывах, - добавляем -1. В итоге если \"положительных\" слов встретилось больше, то \"оценка\" отзыва больше 0, значит, он положительный, если больше \"отрицательных\" слов, то \"оценка\" меньше 0, следовательно, отзыв негативный. На случай если \"оценка\" остается равна 0, я добавила статус нейтральный отзыв."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEFORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = {}\n",
    "ind = 0\n",
    "exp = []\n",
    "real = []\n",
    "\n",
    "for text in check_texts:\n",
    "    c = 0\n",
    "    tokenized_text = [w.lower() for w in word_tokenize(text) if w.isalpha()]\n",
    "    filt_words = [w for w in tokenized_text if w not in sw]\n",
    "    lemmas = [morph.parse(w)[0].normal_form for w in filt_words]\n",
    "    for lemma in lemmas:\n",
    "        if lemma in pro_prep:\n",
    "            c += 1\n",
    "        elif lemma in contra_prep:\n",
    "            c -= 1\n",
    "\n",
    "    if c > 0:\n",
    "        res[ind] = 'pro'\n",
    "        ind += 1\n",
    "    elif c < 0:\n",
    "        res[ind] = 'contra'\n",
    "        ind += 1\n",
    "    else:\n",
    "        res[ind] = 'neutral'\n",
    "        ind += 1\n",
    "\n",
    "for i, j in for_check.items():\n",
    "    exp.append(j)\n",
    "for i, j in res.items():\n",
    "    real.append(j)\n",
    "accuracy_score(real, exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AFTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = {}\n",
    "ind = 0\n",
    "exp = []\n",
    "real = []\n",
    "\n",
    "for text in check_texts:\n",
    "    c = 0\n",
    "    chunks = chunker(text)\n",
    "    for ch in chunks:\n",
    "        if ch in pos_ng:\n",
    "            c += 1\n",
    "        elif ch in neg_ng:\n",
    "            c -= 1\n",
    "    tokenized_text = [w.lower() for w in word_tokenize(text) if w.isalpha()]\n",
    "    filt_words = [w for w in tokenized_text if w not in sw]\n",
    "    lemmas = [morph.parse(w)[0].normal_form for w in filt_words]\n",
    "    for lemma in lemmas:\n",
    "        if lemma in pro_prep:\n",
    "            c += 1\n",
    "        elif lemma in contra_prep:\n",
    "            c -= 1\n",
    "\n",
    "    if c > 0:\n",
    "        res[ind] = 'pro'\n",
    "        ind += 1\n",
    "    elif c < 0:\n",
    "        res[ind] = 'contra'\n",
    "        ind += 1\n",
    "    else:\n",
    "        res[ind] = 'neutral'\n",
    "        ind += 1\n",
    "\n",
    "for i, j in for_check.items():\n",
    "    exp.append(j)\n",
    "for i, j in res.items():\n",
    "    real.append(j)\n",
    "accuracy_score(real, exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В моем случае accuracy не поменялась, так как она и была 1.0, но зато можно точно сказать, что качество не упало, значит, выбор именно этих ngramm не является совсем плохим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предложения:\n",
    "1. Возможно, если использовать еще и tfidf, то результат будет более точным, хотя на моих данных это не особо можно проверить, так как у меня accuracy 1.0 \n",
    "2. На самом деле, оценка 3 звезды довольно сложная для оценивания, так как в целом 3 звезды означает среднее качество, не отличное, но и не плохое, поэтому, возможно, стоит добавить отдельно еще и слова, которые встречаются в нейтральных отзывах, создать третью категорию, нейтральный отзыв\n",
    "3. Еще на том сайте, на котором я брала отзывы, есть рейтинг отзыва и рейтинг пользователя, можно как-то использовать это (например, брать только те отзывы, у которых положительный рейтинг, или отзывы от пользователей с высоким рейтингом)\n",
    "4. Можно сначала взять отзывы с 5 и 1 звездами и найти там какие-то ключевые слова, затем сравнить эти слова с ключевыми словами для отзывов с 4 и 2 звездами соответственно, и те слова, которые совпадут (и не попадутся среди слов для \"полярных\" отзывов), будут иметь больший вес в нашей функции / будут использоваться только эти слова. Если углубиться еще сильнее, то можно провести сравнение для пар 1 и 4 звезды и 5 и 2 звезды, так как можно предположить, что в отзывах с 4 звездами должна встречаться какая-то критика (тк поставили 4, а не 5), а в отзывах с 2 звездами должно встречаться что-то положительное (поставлена не 1 звезда, а 2). Тогда у нас будет еще список положительно и отрицательно окрашенных слов (но хорошо бы их сравнить с полученными до этого списками, чтобы не получилось какого-то случайного пересечения слов)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8dc2e76d637f38cb534d007662ee080514e23b3d0d0dd6e2f651136baa4a9222"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
